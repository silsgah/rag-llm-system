# ğŸš€ Production-Ready RAG System with LLM Fine-Tuning

[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/)
[![Poetry](https://img.shields.io/badge/dependency-poetry-blue)](https://python-poetry.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

> A complete, production-ready RAG (Retrieval-Augmented Generation) system with custom LLM fine-tuning, built following industry best practices. Based on the [LLM Engineer's Handbook](https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/).

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Architecture](#-architecture)
- [Quick Start](#-quick-start)
- [Installation](#-installation)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [Pipelines](#-pipelines)
- [Configuration](#-configuration)
- [Deployment](#-deployment)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)

---

## ğŸ¯ Overview

This project demonstrates how to build a **complete LLM-powered system** from scratch, including:

- ğŸ” **Data Collection**: Web crawlers for LinkedIn, Medium, GitHub, and news sources
- âš™ï¸ **Feature Engineering**: Document processing, chunking, and embedding generation
- ğŸ¤– **LLM Fine-Tuning**: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO)
- ğŸ“Š **RAG System**: Advanced retrieval with query expansion, self-querying, and reranking
- ğŸš€ **Production Deployment**: AWS SageMaker, Docker, CI/CD with GitHub Actions
- ğŸ“ˆ **Monitoring**: Comprehensive tracking with Comet ML and Opik

**Use Cases:**
- Build AI assistants that learn from your content
- Create personalized chatbots
- Implement enterprise knowledge bases
- Research LLM fine-tuning and RAG systems

---

## âœ¨ Features

### ğŸ”§ Core Capabilities

- **Multi-Source Data Collection**: Automated crawlers for web content
- **Vector Database**: Qdrant for efficient similarity search
- **Advanced RAG**:
  - Query expansion for better recall
  - Self-querying for metadata extraction
  - Cross-encoder reranking for precision
- **Custom Fine-Tuning**: Train on your own data using SFT and DPO
- **Production-Ready**: Scalable deployment on AWS SageMaker
- **Full Observability**: Experiment tracking and prompt monitoring

### ğŸ—ï¸ Technical Stack

| Component | Technology |
|-----------|-----------|
| **Language** | Python 3.11 |
| **LLM** | Llama 3.1-8B (fine-tuned) |
| **Embeddings** | sentence-transformers/all-MiniLM-L6-v2 |
| **Vector DB** | Qdrant Cloud |
| **Document DB** | MongoDB Atlas |
| **Orchestration** | ZenML |
| **Training** | AWS SageMaker + Unsloth |
| **API Framework** | FastAPI |
| **Monitoring** | Comet ML + Opik |
| **Deployment** | Docker + AWS |

---

## ğŸ›ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA COLLECTION                           â”‚
â”‚  Web Crawlers â†’ MongoDB (Raw Documents)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FEATURE ENGINEERING                         â”‚
â”‚  Clean â†’ Chunk â†’ Embed â†’ Qdrant (Vector Store)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DATASET GENERATION                          â”‚
â”‚  Generate Instruct & Preference Datasets                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM FINE-TUNING                           â”‚
â”‚  SFT Training â†’ DPO Training â†’ Model Evaluation              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INFERENCE (RAG)                           â”‚
â”‚  Query â†’ Retrieve â†’ Rerank â†’ Generate Response               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**RAG Pipeline Detail:**
```
User Query
    â†“
[Self-Query] â†’ Extract metadata (author, topic)
    â†“
[Query Expansion] â†’ Generate 3 query variations
    â†“
[Embedding] â†’ Convert to vector (384-dim)
    â†“
[Vector Search] â†’ Query Qdrant (articles, posts, repos)
    â†“
[Reranking] â†’ Cross-encoder scoring
    â†“
[LLM Generation] â†’ Final response
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11
- Poetry (dependency manager)
- Docker (for local services)
- OpenAI API key
- Hugging Face account
- Comet ML account

### 5-Minute Setup

```bash
# 1. Clone repository
git clone <your-repo-url>
cd <repo-name>

# 2. Install dependencies
poetry install

# 3. Configure environment
cp .env.example .env
# Edit .env with your API keys

# 4. Start local infrastructure
poetry poe local-infrastructure-up

# 5. Run data pipeline
poetry poe run-digital-data-etl
poetry poe run-feature-engineering-pipeline

# 6. Test RAG
poetry poe call-rag-retrieval-module
```

**You're ready!** ğŸ‰

---

## ğŸ“¦ Installation

### Step 1: Environment Setup

```bash
# Using pyenv (recommended)
pyenv install 3.11.8
pyenv local 3.11.8

# Verify
python --version  # Should show 3.11.x
```

### Step 2: Install Dependencies

```bash
# Install Poetry
curl -sSL https://install.python-poetry.org | python3 -

# Install project dependencies
poetry install

# Activate environment
poetry shell
```

### Step 3: Configure Environment Variables

Create `.env` file:

```bash
# OpenAI (Required)
OPENAI_API_KEY=sk-...
OPENAI_MODEL_ID=gpt-4o-mini

# Hugging Face (Required)
HUGGINGFACE_ACCESS_TOKEN=hf_...

# Comet ML (Required for training/monitoring)
COMET_API_KEY=...
COMET_PROJECT=twin

# MongoDB Atlas (Required)
DATABASE_HOST=mongodb+srv://user:pass@cluster.mongodb.net

# Qdrant Cloud (Required)
USE_QDRANT_CLOUD=true
QDRANT_CLOUD_URL=https://your-cluster.qdrant.io
QDRANT_APIKEY=...

# AWS (Required for training/deployment)
AWS_REGION=eu-north-1
AWS_ACCESS_KEY=...
AWS_SECRET_KEY=...
AWS_ARN_ROLE=arn:aws:iam::...
```

### Step 4: Start Local Services

```bash
# Start MongoDB + Qdrant + ZenML
poetry poe local-infrastructure-up

# Access ZenML dashboard
open http://localhost:8237
```

---

## ğŸ’» Usage

### Data Pipeline

```bash
# Collect data from web sources
poetry poe run-digital-data-etl

# Process and embed documents
poetry poe run-feature-engineering-pipeline

# Generate training datasets
poetry poe run-generate-instruct-datasets-pipeline
poetry poe run-generate-preference-datasets-pipeline

# Or run all at once
poetry poe run-end-to-end-data-pipeline
```

### RAG System

```bash
# Test retrieval
poetry poe call-rag-retrieval-module

# Start API server
poetry poe run-inference-ml-service

# Test API
curl -X POST http://localhost:8000/rag \
  -H "Content-Type: application/json" \
  -d '{"query": "Explain RAG systems"}'
```

### Training (Requires AWS)

```bash
# Setup AWS SageMaker
poetry install --with aws
poetry poe create-sagemaker-role
poetry poe create-sagemaker-execution-role

# Fine-tune with SFT
poetry poe run-training-pipeline

# Fine-tune with DPO (update configs/training.yaml first)
poetry poe run-training-pipeline

# Evaluate models
poetry poe run-evaluation-pipeline

# Deploy to SageMaker
poetry poe deploy-inference-endpoint
poetry poe test-sagemaker-endpoint
```

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ rag_llm_system/          # Main package (Domain-Driven Design)
â”‚   â”œâ”€â”€ domain/              # Core entities (documents, chunks, queries)
â”‚   â”œâ”€â”€ application/         # Business logic
â”‚   â”‚   â”œâ”€â”€ crawlers/        # Web scrapers
â”‚   â”‚   â”œâ”€â”€ rag/             # RAG implementation
â”‚   â”‚   â”œâ”€â”€ preprocessing/   # Data processing
â”‚   â”‚   â”œâ”€â”€ dataset/         # Dataset generation
â”‚   â”‚   â””â”€â”€ networks/        # ML models
â”‚   â”œâ”€â”€ model/               # LLM training & inference
â”‚   â”‚   â”œâ”€â”€ finetuning/      # SFT & DPO training
â”‚   â”‚   â”œâ”€â”€ evaluation/      # Model evaluation
â”‚   â”‚   â””â”€â”€ inference/       # Deployment
â”‚   â””â”€â”€ infrastructure/      # External integrations
â”‚       â”œâ”€â”€ db/              # MongoDB & Qdrant
â”‚       â””â”€â”€ aws/             # SageMaker deployment
â”‚
â”œâ”€â”€ pipelines/               # ZenML ML pipelines
â”œâ”€â”€ steps/                   # Reusable pipeline steps
â”œâ”€â”€ tools/                   # Utility scripts
â”‚   â”œâ”€â”€ run.py              # Pipeline executor
â”‚   â”œâ”€â”€ rag.py              # RAG demo
â”‚   â””â”€â”€ ml_service.py       # API server
â”œâ”€â”€ configs/                 # Configuration files
â”œâ”€â”€ tests/                   # Test suite
â”œâ”€â”€ .env                     # Environment variables (create from .env.example)
â””â”€â”€ pyproject.toml          # Dependencies
```

### Key Files

| File | Description |
|------|-------------|
| `rag_llm_system/settings.py` | Configuration management |
| `rag_llm_system/application/rag/retriever.py` | RAG implementation |
| `rag_llm_system/model/finetuning/finetune.py` | Training logic |
| `tools/rag.py` | RAG demo script |
| `tools/ml_service.py` | FastAPI server |

---

## ğŸ”„ Pipelines

### Available Commands

| Pipeline | Command | Description |
|----------|---------|-------------|
| **ETL** | `poetry poe run-digital-data-etl` | Collect web data |
| **Feature Engineering** | `poetry poe run-feature-engineering-pipeline` | Process & embed |
| **Instruct Dataset** | `poetry poe run-generate-instruct-datasets-pipeline` | Generate Q&A pairs |
| **Preference Dataset** | `poetry poe run-generate-preference-datasets-pipeline` | Generate DPO data |
| **Training** | `poetry poe run-training-pipeline` | Fine-tune LLM |
| **Evaluation** | `poetry poe run-evaluation-pipeline` | Evaluate model |
| **RAG Test** | `poetry poe call-rag-retrieval-module` | Test retrieval |
| **API Server** | `poetry poe run-inference-ml-service` | Start API |

### Pipeline Workflow

**Full End-to-End:**
```bash
# Data phase (local)
poetry poe run-end-to-end-data-pipeline

# Training phase (AWS SageMaker required)
poetry poe run-training-pipeline
poetry poe run-evaluation-pipeline

# Deployment phase
poetry poe deploy-inference-endpoint
poetry poe run-inference-ml-service
```

---

## âš™ï¸ Configuration

### Customize Data Sources

Edit `configs/digital_data_etl_*.yaml`:

```yaml
links:
  - https://medium.com/@yourusername
  - https://dev.to/yourusername
  - https://github.com/yourusername
```

### Customize Training

Edit `configs/training.yaml`:

```yaml
finetuning_type: "sft"  # or "dpo"
num_train_epochs: 3
per_device_train_batch_size: 2
learning_rate: 3e-4
model_id: "meta-llama/Llama-3.1-8B-Instruct"
```

### Customize RAG

Edit `rag_llm_system/settings.py`:

```python
TEXT_EMBEDDING_MODEL_ID = "sentence-transformers/all-MiniLM-L6-v2"
RERANKING_CROSS_ENCODER_MODEL_ID = "cross-encoder/ms-marco-MiniLM-L-4-v2"
OPENAI_MODEL_ID = "gpt-4o-mini"
```

---

## ğŸš¢ Deployment

### Local Development

```bash
# Start services
poetry poe local-infrastructure-up

# Run API
poetry poe run-inference-ml-service
```

### AWS SageMaker (Production)

```bash
# 1. Setup
poetry install --with aws
poetry poe create-sagemaker-role
poetry poe create-sagemaker-execution-role

# 2. Train model
poetry poe run-training-pipeline

# 3. Deploy endpoint
poetry poe deploy-inference-endpoint

# 4. Test
poetry poe test-sagemaker-endpoint

# 5. Delete when done
poetry poe delete-inference-endpoint
```

### Docker

```bash
# Build image
poetry poe build-docker-image

# Run pipeline
poetry poe run-docker-end-to-end-data-pipeline
```

---

## ğŸ› Troubleshooting

### Common Issues

**Issue: MongoDB Authentication Failed**
```bash
# Check credentials in .env
DATABASE_HOST=mongodb+srv://user:pass@cluster.mongodb.net

# Verify network access in MongoDB Atlas
# Add IP: 0.0.0.0/0 (for testing)
```

**Issue: Qdrant 404 Not Found**
```bash
# Ensure URL has no :6333 port for cloud
QDRANT_CLOUD_URL=https://cluster.qdrant.io  # âœ“ Correct
QDRANT_CLOUD_URL=https://cluster.qdrant.io:6333  # âœ— Wrong

# Re-run feature engineering
poetry poe run-feature-engineering-pipeline
```

**Issue: OpenAI Rate Limit**
```bash
# Reduce batch size in dataset generation
# Or wait 60 seconds between retries
```

**Issue: Pydantic Version Conflict**
```bash
poetry add "pydantic>=2.8.0,<2.9.0"
poetry install
```

### Debug Mode

```bash
# Check data status
poetry run python check_data_status.py

# View ZenML runs
open http://localhost:8237

# View logs
tail -f logs/app.log
```

### Get Help

- **Documentation**: Check `RAG_MODULE_TRACE.md` for detailed flow
- **Issues**: Open a GitHub issue
- **Original Book**: [LLM Engineer's Handbook](https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/)

---

## ğŸ’° Cost Estimate

**One-time full run:**
- AWS SageMaker (training): ~$20
- OpenAI API (dataset generation): ~$3
- MongoDB Atlas (free tier): $0
- Qdrant Cloud (free tier): $0
- **Total**: ~$25

**Monthly (if deployed):**
- SageMaker endpoint: ~$100-200/month
- API calls: Variable

---

## ğŸ“Š Performance

- **RAG Latency**: 2-5 seconds (including reranking)
- **Training Time**: 2-4 hours (SFT on 8B model)
- **Throughput**: ~10 requests/second (SageMaker)

---

## ğŸ§ª Testing

```bash
# Run all tests
poetry poe test

# Lint check
poetry poe lint-check

# Format check
poetry poe format-check
```

---

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file

---

## ğŸ™ Acknowledgments

- Based on [LLM Engineer's Handbook](https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/) by Paul Iusztin and Maxime Labonne
- Built with [ZenML](https://zenml.io/), [Unsloth](https://github.com/unslothai/unsloth), and [LangChain](https://langchain.com/)

---

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

---

## ğŸ“ Contact

- **Author**: silsgah
- **GitHub**: [@silsgah](https://github.com/silsgah)

---

**â­ Star this repo if you found it helpful!**

