# RAG System Complete Architecture Documentation

## Table of Contents
1. [Why Two Servers? Inference vs Main API](#1-why-two-servers)
2. [Complete Data Flow](#2-complete-data-flow)
3. [Vector Database Architecture (FAISS vs Qdrant)](#3-vector-database-architecture)
4. [RAG Retrieval Deep Dive](#4-rag-retrieval-deep-dive)
5. [Deployment Architecture](#5-deployment-architecture)

---

## 1. Why Two Servers?

Your system has **two separate concerns** that require different infrastructure:

### Server 1: Main API (RAG Logic)
**File:** `tools/ml_service.py` â†’ `rag_llm_system/infrastructure/inference_pipeline_api.py`

**What it does:**
- Accepts user queries via `/rag` endpoint
- Runs RAG retrieval pipeline:
  - Self-query (extract author)
  - Query expansion (generate variations)
  - Vector search (query Qdrant/FAISS)
  - Reranking (cross-encoder)
  - Context formatting
- Calls the LLM inference service
- Returns final answer

**Resource needs:**
- CPU: Low-medium (text processing)
- RAM: 1-2GB (no model weights)
- GPU: NOT required
- Dependencies: FastAPI, sentence-transformers (embeddings), langchain

**Why separate?**
- Lightweight service, can run anywhere
- Scales horizontally (add more instances)
- No expensive GPU needed

---

### Server 2: Inference Server (LLM Model)
**File:** `rag_llm_system/infrastructure/local/local_deploy.py` or SageMaker

**What it does:**
- Hosts the actual LLM model (8B parameters)
- Exposes `/infer` endpoint
- Generates text from prompts
- Returns generated text

**Resource needs:**
- CPU: High (if no GPU)
- RAM: 16-32GB (model weights ~16GB for FP16)
- GPU: STRONGLY recommended (A10G/T4)
- Dependencies: transformers, torch, vLLM/TGI

**Why separate?**
- Heavy resource requirements
- GPU acceleration critical for speed
- Expensive to scale (GPU instances)
- Can be shared by multiple API instances

---

### Communication Flow

```
User Request
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Main API Server (Lightweight) â”‚  â† Render.com ($7/mo)
â”‚  - FastAPI                     â”‚
â”‚  - RAG retrieval logic         â”‚
â”‚  - Vector DB queries           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ HTTP POST /infer
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Inference Server (GPU Heavy)  â”‚  â† Modal/RunPod ($10-20/mo)
â”‚  - LLM model (8B params)       â”‚  OR SageMaker ($864/mo)
â”‚  - Text generation             â”‚  OR Local (your setup)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Generated Answer
    â†“
User Response
```

### Why You Need Both for Cloud Deployment

**Scenario 1: All-in-One Server**
```
Problem:
- Need GPU instance for LLM â†’ $100+/month minimum
- Can't scale main API independently
- Wasted GPU cycles during retrieval/processing
```

**Scenario 2: Separated Servers (Your Architecture)**
```
Solution:
- Main API on cheap CPU instance â†’ $7/month
- Inference server on GPU â†’ $10-20/month (or local)
- Scale each independently
- Total: $17-27/month vs $864/month (SageMaker)
```

---

## 2. Complete Data Flow

### Phase 1: Data Ingestion

```
Web Sources (Medium, LinkedIn, GitHub)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Crawler Dispatcher                     â”‚
â”‚  File: application/crawlers/dispatcher.py â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Specialized Crawlers (Selenium)        â”‚
â”‚  - MediumCrawler                        â”‚
â”‚  - LinkedInCrawler                      â”‚
â”‚  - GithubCrawler                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MongoDB Atlas (Raw Storage)            â”‚
â”‚  Collections:                           â”‚
â”‚  - articles  (Medium, blog posts)       â”‚
â”‚  - posts     (LinkedIn posts)           â”‚
â”‚  - repositories (GitHub repos)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Schema Example (Article):
{
    "platform": "medium",
    "link": "https://medium.com/@paul/article",
    "content": {
        "title": "Understanding RAG Systems",
        "subtitle": "A complete guide",
        "text": "<full article content>"
    },
    "author_id": "uuid-abc-123",
    "author_full_name": "Paul Iusztin"
}
```

**Run with:** `poetry poe run-digital-data-etl`

---

### Phase 2: Feature Engineering

```
MongoDB Raw Documents
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Query Data Warehouse          â”‚
â”‚  File: steps/feature_engineering/       â”‚
â”‚        query_data_warehouse.py          â”‚
â”‚  - Fetches docs by author_id           â”‚
â”‚  - Parallel queries (ThreadPoolExecutor)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: Cleaning Dispatcher            â”‚
â”‚  File: application/preprocessing/       â”‚
â”‚        cleaning_data_handlers.py        â”‚
â”‚                                         â”‚
â”‚  ArticleCleaningHandler:                â”‚
â”‚  - Remove HTML tags                     â”‚
â”‚  - Normalize whitespace                 â”‚
â”‚  - Fix encoding issues                  â”‚
â”‚                                         â”‚
â”‚  PostCleaningHandler:                   â”‚
â”‚  - Remove social media formatting       â”‚
â”‚  - Clean hashtags                       â”‚
â”‚                                         â”‚
â”‚  RepositoryCleaningHandler:             â”‚
â”‚  - Process README files                 â”‚
â”‚  - Extract code documentation           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: Chunking Dispatcher            â”‚
â”‚  File: application/preprocessing/       â”‚
â”‚        chunking_data_handlers.py        â”‚
â”‚                                         â”‚
â”‚  Strategy by Document Type:             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Posts:                          â”‚   â”‚
â”‚  â”‚ - 250 tokens per chunk          â”‚   â”‚
â”‚  â”‚ - 25 token overlap              â”‚   â”‚
â”‚  â”‚ - Small chunks (short content)  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Articles:                       â”‚   â”‚
â”‚  â”‚ - 1000-2000 chars per chunk     â”‚   â”‚
â”‚  â”‚ - Semantic chunking (paragraphs)â”‚   â”‚
â”‚  â”‚ - Preserve meaning boundaries   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Repositories:                   â”‚   â”‚
â”‚  â”‚ - 1500 tokens per chunk         â”‚   â”‚
â”‚  â”‚ - 100 token overlap             â”‚   â”‚
â”‚  â”‚ - Code-aware splitting          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                         â”‚
â”‚  Implementation:                        â”‚
â”‚  langchain.text_splitter.               â”‚
â”‚    RecursiveCharacterTextSplitter       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: Embedding Generation           â”‚
â”‚  File: application/preprocessing/       â”‚
â”‚        embedding_data_handlers.py       â”‚
â”‚                                         â”‚
â”‚  Model: sentence-transformers/          â”‚
â”‚         all-MiniLM-L6-v2                â”‚
â”‚                                         â”‚
â”‚  Process:                               â”‚
â”‚  1. Load chunks in batches of 10       â”‚
â”‚  2. Generate embeddings:                â”‚
â”‚     "RAG systems combine..." â†’          â”‚
â”‚     [0.123, -0.456, ..., 0.789]        â”‚
â”‚     (384 dimensions)                    â”‚
â”‚  3. Store embedding + metadata          â”‚
â”‚                                         â”‚
â”‚  Device: CPU (configurable to GPU)      â”‚
â”‚  Singleton: One model instance          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 5: Vector Database Storage        â”‚
â”‚  File: steps/feature_engineering/       â”‚
â”‚        load_to_vector_db.py             â”‚
â”‚                                         â”‚
â”‚  Stores TWO types:                      â”‚
â”‚  1. Cleaned Documents (no embeddings)   â”‚
â”‚     - For dataset generation            â”‚
â”‚                                         â”‚
â”‚  2. Embedded Chunks (with vectors)      â”‚
â”‚     - For RAG retrieval                 â”‚
â”‚                                         â”‚
â”‚  Collections Created:                   â”‚
â”‚  - cleaned_articles                     â”‚
â”‚  - cleaned_posts                        â”‚
â”‚  - cleaned_repositories                 â”‚
â”‚  - article_chunks_embedded              â”‚
â”‚  - post_chunks_embedded                 â”‚
â”‚  - repository_chunks_embedded           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Run with:** `poetry poe run-feature-engineering-pipeline`

---

### Phase 3: RAG Retrieval (Query Time)

```
User Query: "How does Paul implement RAG?"
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Self-Query                     â”‚
â”‚  File: application/rag/self_query.py    â”‚
â”‚                                         â”‚
â”‚  Purpose: Extract author metadata       â”‚
â”‚                                         â”‚
â”‚  Process:                               â”‚
â”‚  1. LLM call (GPT-4o-mini):             â”‚
â”‚     "Extract author from query..."      â”‚
â”‚  2. Extracts: "Paul Iusztin"            â”‚
â”‚  3. Splits: first_name="Paul",          â”‚
â”‚             last_name="Iusztin"         â”‚
â”‚  4. Query MongoDB for user              â”‚
â”‚  5. Return: author_id="abc-123"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: Query Expansion                â”‚
â”‚  File: application/rag/query_expansion.pyâ”‚
â”‚                                         â”‚
â”‚  Purpose: Generate query variations     â”‚
â”‚          for better recall              â”‚
â”‚                                         â”‚
â”‚  Process:                               â”‚
â”‚  1. LLM call (GPT-4o-mini):             â”‚
â”‚     "Generate 2 more variations..."     â”‚
â”‚  2. Original: "How does Paul implement  â”‚
â”‚               RAG?"                     â”‚
â”‚  3. Variation 1: "Paul's RAG            â”‚
â”‚                   implementation        â”‚
â”‚                   approach"             â”‚
â”‚  4. Variation 2: "RAG system            â”‚
â”‚                   architecture by Paul" â”‚
â”‚                                         â”‚
â”‚  Result: 3 queries for parallel search  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: Vector Search (Parallel)       â”‚
â”‚  File: application/rag/retriever.py     â”‚
â”‚                                         â”‚
â”‚  For EACH of 3 queries:                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 1. Convert to embedding:          â”‚ â”‚
â”‚  â”‚    query â†’ [0.1, -0.4, ...] (384) â”‚ â”‚
â”‚  â”‚                                   â”‚ â”‚
â”‚  â”‚ 2. Search 3 collections (parallel):â”‚ â”‚
â”‚  â”‚    - article_chunks_embedded      â”‚ â”‚
â”‚  â”‚    - post_chunks_embedded         â”‚ â”‚
â”‚  â”‚    - repository_chunks_embedded   â”‚ â”‚
â”‚  â”‚                                   â”‚ â”‚
â”‚  â”‚ 3. Each returns k//3 results      â”‚ â”‚
â”‚  â”‚    (e.g., k=9 â†’ 3 per collection) â”‚ â”‚
â”‚  â”‚                                   â”‚ â”‚
â”‚  â”‚ 4. Filter by author_id="abc-123"  â”‚ â”‚
â”‚  â”‚                                   â”‚ â”‚
â”‚  â”‚ 5. Similarity: Cosine distance    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                         â”‚
â”‚  Uses ThreadPoolExecutor for            â”‚
â”‚  concurrent searching                   â”‚
â”‚                                         â”‚
â”‚  Total results: 3 queries Ã— 9 chunks    â”‚
â”‚                = 27 chunks              â”‚
â”‚  Deduplicate by chunk.id â†’ 9 unique     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: Reranking                      â”‚
â”‚  File: application/rag/reranking.py     â”‚
â”‚                                         â”‚
â”‚  Model: cross-encoder/                  â”‚
â”‚         ms-marco-MiniLM-L-4-v2          â”‚
â”‚                                         â”‚
â”‚  Purpose: Precision over recall         â”‚
â”‚                                         â”‚
â”‚  Process:                               â”‚
â”‚  1. Create pairs:                       â”‚
â”‚     [("query", chunk1.content),         â”‚
â”‚      ("query", chunk2.content),         â”‚
â”‚      ...]                               â”‚
â”‚                                         â”‚
â”‚  2. Cross-encoder scores each pair:     â”‚
â”‚     - Joint encoding (not separate)     â”‚
â”‚     - Score: 0-1 similarity             â”‚
â”‚                                         â”‚
â”‚  3. Sort by score (descending)          â”‚
â”‚                                         â”‚
â”‚  4. Return top_k=3 chunks               â”‚
â”‚                                         â”‚
â”‚  Difference from bi-encoder:            â”‚
â”‚  - Bi-encoder: Fast, separate vectors   â”‚
â”‚  - Cross-encoder: Slow, more accurate   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 5: Context Formatting             â”‚
â”‚                                         â”‚
â”‚  Combine top 3 chunks into context:     â”‚
â”‚                                         â”‚
â”‚  Context:                               â”‚
â”‚  ---                                    â”‚
â”‚  Chunk 1: "In my RAG implementation..." â”‚
â”‚  Chunk 2: "Vector databases allow..."   â”‚
â”‚  Chunk 3: "Retrieval augmented..."      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 6: LLM Generation                 â”‚
â”‚  File: infrastructure/                  â”‚
â”‚        inference_pipeline_api.py        â”‚
â”‚                                         â”‚
â”‚  Prompt Template:                       â”‚
â”‚  """                                    â”‚
â”‚  Context: {context}                     â”‚
â”‚                                         â”‚
â”‚  Question: {query}                      â”‚
â”‚                                         â”‚
â”‚  Answer:                                â”‚
â”‚  """                                    â”‚
â”‚                                         â”‚
â”‚  LLM Options:                           â”‚
â”‚  A. SageMaker Endpoint                  â”‚
â”‚     - Model: TwinLlama-3.1-8B-DPO       â”‚
â”‚     - Instance: ml.g5.2xlarge           â”‚
â”‚     - Cost: $864/month                  â”‚
â”‚                                         â”‚
â”‚  B. Local Endpoint (YOUR SETUP)         â”‚
â”‚     - Model: Any HF model               â”‚
â”‚     - Server: local_deploy.py           â”‚
â”‚     - Cost: $0 (local GPU)              â”‚
â”‚                                         â”‚
â”‚  C. OpenAI API                          â”‚
â”‚     - Model: gpt-4o-mini                â”‚
â”‚     - Cost: ~$5-10/month                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Generated Answer: "Paul implements RAG by..."
    â†“
Return to User
```

**Test with:** `poetry poe call-rag-retrieval-module`

---

## 3. Vector Database Architecture

Your system supports **3 vector databases** via Adapter Pattern:

### Factory Pattern Implementation

**File:** `rag_llm_system/infrastructure/vector_stores/factory.py`

```python
VectorStoreFactory.create(backend, **kwargs):
    Supported backends:
    - "qdrant" â†’ QdrantAdapter (Production default)
    - "faiss"  â†’ FAISSAdapter (High-performance)
    - "chroma" â†’ ChromaAdapter (Development)
```

---

### 3.1 Qdrant (Default Production)

**File:** `infrastructure/vector_stores/qdrant_adapter.py`

**Architecture:**
```
Qdrant Cloud (EU-West-2)
    â†“
QdrantDatabaseConnector (Singleton)
    â†“
6 Collections:
    - cleaned_articles
    - cleaned_posts
    - cleaned_repositories
    - article_chunks_embedded  â† Used for RAG
    - post_chunks_embedded     â† Used for RAG
    - repository_chunks_embedded â† Used for RAG
```

**Features:**
- Native metadata filtering: `author_id="abc-123"`
- Cosine similarity search
- Cloud-managed (or self-hosted)
- Distributed deployment
- Automatic upsert (insert or update)

**Configuration:**
```python
# .env
USE_QDRANT_CLOUD=true
QDRANT_CLOUD_URL=https://xxx.qdrant.io
QDRANT_APIKEY=your-key

# For local:
USE_QDRANT_CLOUD=false
# Uses docker-compose.yml (localhost:6333)
```

**When to use:**
- Production deployment
- Need managed service
- Multiple users/applications
- Requires metadata filtering

---

### 3.2 FAISS (High-Performance Alternative)

**File:** `infrastructure/vector_stores/faiss_adapter.py`

**Architecture:**
```
In-Memory Hybrid System:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FAISS Index (IndexFlatIP)              â”‚
â”‚  - Stores: Vector embeddings only       â”‚
â”‚  - Type: Inner Product (cosine sim)     â”‚
â”‚  - Backend: NumPy/GPU accelerated       â”‚
â”‚  - Size: ~1.5GB for 1M vectors (384-dim)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         +
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python Dict (Metadata Store)           â”‚
â”‚  - Stores: chunk.id â†’ {content, author, â”‚
â”‚            platform, ...}               â”‚
â”‚  - Size: ~500MB for 1M chunks          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pickle Persistence                     â”‚
â”‚  - index.faiss (FAISS index)            â”‚
â”‚  - metadata.pkl (Python dict)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**How FAISS is Used:**

1. **Index Creation:**
```python
# When feature engineering runs:
import faiss
index = faiss.IndexFlatIP(384)  # 384 = embedding dimensions

# Add vectors:
vectors = np.array([[0.1, 0.2, ...], [...]])  # Shape: (N, 384)
index.add(vectors)

# Save to disk:
faiss.write_index(index, "index.faiss")
```

2. **Search (at query time):**
```python
# Convert query to vector:
query_vector = embedding_model.encode("How does RAG work?")  # (384,)

# Search FAISS:
k = 9  # top 9 results
distances, indices = index.search(query_vector.reshape(1, -1), k)

# distances: [0.95, 0.87, 0.82, ...]  # similarity scores
# indices:   [123, 456, 789, ...]     # vector IDs

# Fetch metadata:
results = [metadata[idx] for idx in indices[0]]
```

3. **Post-Filtering (Limitation):**
```python
# FAISS doesn't support native metadata filtering
# So we search MORE, then filter:

k_with_buffer = 50  # Search 50 instead of 9
distances, indices = index.search(query_vector, k_with_buffer)

# Post-filter by author:
filtered = [
    metadata[idx]
    for idx in indices[0]
    if metadata[idx]["author_id"] == "abc-123"
][:9]  # Take top 9 after filtering
```

**Performance:**
```
Benchmark (1M vectors, 384-dim):

FAISS (CPU):        0.5-2ms per query
FAISS (GPU):        0.1-0.5ms per query
Qdrant Cloud:       10-50ms per query (network latency)
ChromaDB:           5-15ms per query
```

**When to use FAISS:**
- Offline/edge deployment
- GPU available
- Don't need complex metadata filtering
- Maximum search speed critical
- Large-scale batch processing
- Research/experimentation

**Limitations:**
- All data must fit in RAM
- No native distributed deployment
- Manual save/load required
- Post-filtering (less efficient with many filters)

---

### 3.3 ChromaDB (Development)

**File:** `infrastructure/vector_stores/chroma_adapter.py`

**Features:**
- Python-native (no external server)
- Automatic persistence to disk
- Built-in metadata filtering
- Simple API

**When to use:**
- Development/prototyping
- Small datasets (<100K vectors)
- Simple deployment

---

### Comparison Table

| Feature | Qdrant | FAISS | ChromaDB |
|---------|--------|-------|----------|
| **Speed** | Fast | Very Fast | Medium |
| **Scale** | 100M+ | 10M+ (RAM) | 1M |
| **Metadata Filter** | Native | Post-filter | Native |
| **Persistence** | Automatic | Manual | Automatic |
| **Distributed** | Yes | No | No |
| **GPU Support** | No | Yes | No |
| **Cloud Option** | Yes | No | No |
| **Best For** | Production | Performance | Development |

---

## 4. RAG Retrieval Deep Dive

### Why Multiple Retrieval Stages?

Each stage improves different aspects:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Self-Query: IMPROVES PRECISION                              â”‚
â”‚ - Without: "How does Paul implement RAG?" searches ALL docs â”‚
â”‚ - With: Filters to only Paul's documents                    â”‚
â”‚ - Impact: 10x fewer irrelevant results                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query Expansion: IMPROVES RECALL                            â”‚
â”‚ - Without: Single query may miss relevant docs             â”‚
â”‚ - With: 3 variations capture different phrasings            â”‚
â”‚ - Impact: 30-50% more relevant docs retrieved              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector Search: FAST RETRIEVAL                               â”‚
â”‚ - Bi-encoder (separate embeddings) = very fast              â”‚
â”‚ - Cosine similarity: O(N) with FAISS, O(log N) with Qdrant â”‚
â”‚ - Impact: Retrieves 9-27 candidate chunks in <10ms         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reranking: IMPROVES PRECISION                               â”‚
â”‚ - Cross-encoder (joint encoding) = more accurate            â”‚
â”‚ - Re-scores only top candidates (9-27 chunks)               â”‚
â”‚ - Impact: 20-40% improvement in relevance                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Performance Trade-offs

```
Stage          | Latency | Accuracy | When to Skip
---------------|---------|----------|---------------
Self-Query     | ~500ms  | +20%     | No author refs
Query Expansion| ~800ms  | +40%     | Simple queries
Vector Search  | ~10ms   | Baseline | Never
Reranking      | ~200ms  | +30%     | Speed critical

Total RAG latency: ~2-5 seconds (acceptable for chatbots)
```

---

## 5. Deployment Architecture

### Current Local Setup (What You Tested)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LOCAL MACHINE                                             â”‚
â”‚                                                           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Docker Compose                                      â”‚ â”‚
â”‚ â”‚   - MongoDB (localhost:27017)                       â”‚ â”‚
â”‚ â”‚   - Qdrant (localhost:6333)                         â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Inference Server (Port 8000)                        â”‚ â”‚
â”‚ â”‚   File: local_deploy.py                             â”‚ â”‚
â”‚ â”‚   - Loads HuggingFace model                         â”‚ â”‚
â”‚ â”‚   - Endpoint: POST /infer                           â”‚ â”‚
â”‚ â”‚   - GPU/CPU inference                               â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Main API (Port 8001, separate terminal)            â”‚ â”‚
â”‚ â”‚   File: ml_service.py                               â”‚ â”‚
â”‚ â”‚   - RAG retrieval logic                             â”‚ â”‚
â”‚ â”‚   - Calls localhost:8000/infer                      â”‚ â”‚
â”‚ â”‚   - Endpoint: POST /rag                             â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Start commands:**
```bash
# Terminal 1: Infrastructure
poetry poe local-infrastructure-up

# Terminal 2: Inference Server
poetry poe deploy-inference-local  # Port 8000

# Terminal 3: Main API
poetry poe run-inference-ml-service  # Port 8001
```

---

### Cloud Deployment Options

#### Option A: Two Separate Services (Recommended)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CHEAP CLOUD SETUP (~$20-30/month)                         â”‚
â”‚                                                           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Main API â†’ Render.com ($7/mo)                       â”‚ â”‚
â”‚ â”‚   - CPU only                                        â”‚ â”‚
â”‚ â”‚   - 512MB-1GB RAM                                   â”‚ â”‚
â”‚ â”‚   - Scales to 0 when idle                           â”‚ â”‚
â”‚ â”‚   - Endpoint: https://your-api.onrender.com/rag     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â†“ HTTP calls                                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Inference Server â†’ Modal.com ($10-20/mo)            â”‚ â”‚
â”‚ â”‚   - GPU (T4/A10G)                                   â”‚ â”‚
â”‚ â”‚   - 8-16GB RAM                                      â”‚ â”‚
â”‚ â”‚   - Auto-scale to 0                                 â”‚ â”‚
â”‚ â”‚   - Endpoint: https://your-model.modal.run/infer    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Databases (Managed)                                 â”‚ â”‚
â”‚ â”‚   - MongoDB Atlas: FREE (512MB)                     â”‚ â”‚
â”‚ â”‚   - Qdrant Cloud: FREE (1GB vectors)                â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Option B: Keep Inference Local (Cheapest)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HYBRID SETUP (~$7/month)                                  â”‚
â”‚                                                           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Main API â†’ Render.com ($7/mo)                       â”‚ â”‚
â”‚ â”‚   Endpoint: https://your-api.onrender.com/rag       â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â†“ HTTP calls via ngrok tunnel                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ YOUR LOCAL MACHINE                                  â”‚ â”‚
â”‚ â”‚   - Inference server on GPU                         â”‚ â”‚
â”‚ â”‚   - Expose via ngrok/cloudflare tunnel              â”‚ â”‚
â”‚ â”‚   - URL: https://abc123.ngrok.io/infer              â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Summary: Key Architectural Decisions

### 1. Two-Server Architecture
- **Separation of concerns:** Lightweight API vs Heavy inference
- **Independent scaling:** Scale API without GPU costs
- **Cost optimization:** Run GPU only when needed

### 2. Vector Database Flexibility
- **Qdrant:** Production default (managed, scalable)
- **FAISS:** Performance option (10-100x faster, GPU, offline)
- **ChromaDB:** Development option (simple, Python-native)

### 3. Multi-Stage RAG
- **Self-Query:** Filter by author (precision)
- **Query Expansion:** Multiple variations (recall)
- **Vector Search:** Fast candidate retrieval
- **Reranking:** Final precision boost

### 4. Data Flow Stages
1. **Ingestion:** Web â†’ MongoDB (raw)
2. **Processing:** Clean â†’ Chunk â†’ Embed
3. **Storage:** Vector DB (Qdrant/FAISS)
4. **Retrieval:** Multi-stage RAG pipeline
5. **Generation:** LLM inference (local/cloud)

---

## Files Reference

**Core RAG Logic:**
- `rag_llm_system/application/rag/retriever.py` - Main retrieval
- `rag_llm_system/application/rag/self_query.py` - Author extraction
- `rag_llm_system/application/rag/query_expansion.py` - Query variations
- `rag_llm_system/application/rag/reranking.py` - Cross-encoder reranking

**Vector Stores:**
- `infrastructure/vector_stores/qdrant_adapter.py` - Qdrant implementation
- `infrastructure/vector_stores/faiss_adapter.py` - FAISS implementation
- `infrastructure/vector_stores/chroma_adapter.py` - ChromaDB implementation
- `infrastructure/vector_stores/factory.py` - Factory pattern

**Inference:**
- `infrastructure/inference_pipeline_api.py` - Main API `/rag` endpoint
- `infrastructure/local/local_deploy.py` - Local inference server
- `model/inference/local.py` - Local endpoint client

**Pipelines:**
- `pipelines/digital_data_etl.py` - Data ingestion
- `pipelines/feature_engineering.py` - Embedding generation
- `pipelines/end_to_end_data.py` - Complete data flow

This architecture is production-ready and highly flexible!


SAMPLE RESPONSE

âš¡ feature/vector-db-adapters ~/rag-llm-system poetry run poe test-inference-local
Poe => poetry run python -m rag_llm_system.model.inference.localtest
2025-11-14 21:17:15.369 | INFO     | rag_llm_system.settings:load_settings:98 - Loading settings from the ZenML secret store.
2025-11-14 21:17:15.468 | WARNING  | rag_llm_system.settings:load_settings:103 - Failed to load settings from the ZenML secret store. Defaulting to loading the settings from the '.env' file.
2025-11-14 21:17:15.835 | INFO     | rag_llm_system.infrastructure.db.mongo:__new__:20 - Connection to MongoDB with URI successful: mongodb+srv://gahsilas:password123456@cluster0.ujpvp0t.mongodb.net
Skipping import of cpp extensions due to incompatible torch version 2.7.1+cu126 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
PyTorch version 2.7.1 available.
2025-11-14 21:17:20.134 | INFO     | rag_llm_system.infrastructure.db.qdrant:__new__:29 - Connection to Qdrant DB with URI successful: https://24811a25-7844-4c06-a881-c02a2c7a9583.eu-west-2-0.aws.cloud.qdrant.io
HTTP Request: GET https://www.comet.com/api/rest/v2/account-details "HTTP/1.1 200 OK"
HTTP Request: GET https://www.comet.com/api/rest/v2/account-details "HTTP/1.1 200 OK"
HTTP Request: GET https://www.comet.com/api/rest/v2/workspaces "HTTP/1.1 200 OK"
OPIK: Configuration saved to file: /teamspace/studios/this_studio/.opik.config
2025-11-14 21:17:21.559 | INFO     | rag_llm_system.infrastructure.opik_utils:configure_opik:22 - Opik configured successfully.
2025-11-14 21:17:21.559 | INFO     | __main__:local_inference_test:31 - Running inference for text: 'The recent amendment of the agricultural agreement between Morocco and the European Union signifies a noteworthy development in international trade relations. This agreement confirms the applicability of preferential tariffs to Southern Provinces,'
2025-11-14 21:17:21.559 | INFO     | rag_llm_system.model.inference.local:inference:23 - Sending prompt to local inference API: 
You are a content creator. Write what the user asked you to while using the pro...
2025-11-14 21:19:58.783 | INFO     | __main__:local_inference_test:48 - Answer: '
You are a content creator. Write what the user asked you to while using the provided context as the primary source of information for the content.
User query: The recent amendment of the agricultural agreement between Morocco and the European Union signifies a noteworthy development in international trade relations. This agreement confirms the applicability of preferential tariffs to Southern Provinces,
Context: 
             Morocco and the European Union (EU) have recently amended their agricultural agreement, which signifies a noteworthy development in international trade relations. This agreement confirms the applicability of preferential tariffs to the Southern Provinces, including the Western Sahara, as designated by the EU. This is a significant milestone for the EU, as it seeks to strengthen its trade relations with Morocco, a key strategic partner in the region. The agreement also underscores the importance of the EU's commitment to supporting the agricultural sector in Morocco, which is a major contributor to the country's economy. This amendment is expected to provide significant benefits to both parties, as it will facilitate greater market access for Moroccan agricultural products, while also ensuring that the EU can continue to access high-quality agricultural goods from Morocco. This agreement is a testament to the strong partnership between the EU and Morocco, and it is expected to have a positive impact on the economies of both countries, as well as the wider region.'
OPIK: Started logging traces to the "twin" project at https://www.comet.com/opik/silsgah/projects.
HTTP Request: POST https://www.comet.com/opik/api/v1/private/traces "HTTP/1.1 201 Created"
HTTP Request: POST https://www.comet.com/opik/api/v1/private/spans/batch "HTTP/1.1 204 No Content"
âš¡ feature/vector-db-adapters ~/rag-llm-system 

âš¡ feature/vector-db-adapters ~/rag-llm-system poetry run poe deploy-inference-local
Poe => poetry run python -m rag_llm_system.infrastructure.local.local_deploy
2025-11-14 21:12:30.184 | INFO     | rag_llm_system.settings:load_settings:98 - Loading settings from the ZenML secret store.
2025-11-14 21:12:30.426 | WARNING  | rag_llm_system.settings:load_settings:103 - Failed to load settings from the ZenML secret store. Defaulting to loading the settings from the '.env' file.
.mongodb.net
Skipping import of cpp extensions due to incompatible torch version 2.7.1+cu126 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
PyTorch version 2.7.1 available.
2025-11-14 21:12:35.537 | INFO     | rag_llm_system.infrastructure.db.qdrant:__new__:29 - Connection to Qdrant DB with URI successful: https://24811a25-7844-4c06-a881-c02a2c7a9583.eu-west-2-0.aws.cloud.qdrant.io
2025-11-14 21:12:35.859 | INFO     | __main__:<module>:25 - ğŸš€ Starting local model deployment for mlabonne/TwinLlama-3.1-8B-DPO on cuda...
2025-11-14 21:12:35.859 | INFO     | __main__:<module>:30 - âœ… Using authenticated Hugging Face Hub access.
2025-11-14 21:12:35.859 | INFO     | __main__:<module>:33 - ğŸ”„ Loading tokenizer and model from Hugging Face Hub...
tokenizer_config.json: 50.6kB [00:00, 59.0MB/s]
tokenizer.json: 9.09MB [00:00, 164MB/s]
special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 459/459 [00:00<00:00, 3.25MB/s]
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 932/932 [00:00<00:00, 7.88MB/s]
`torch_dtype` is deprecated! Use `dtype` instead!
model.safetensors.index.json: 23.9kB [00:00, 95.9MB/s]
model-00004-of-00004.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.17G/1.17G [00:24<00:00, 47.1MB/s]
model-00001-of-00004.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.98G/4.98G [01:25<00:00, 58.5MB/s]
model-00003-of-00004.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.92G/4.92G [01:26<00:00, 56.8MB/s]
model-00003-of-00004.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 4.71G/4.92G [01:23<00:01, 108MB/s]model-00002-of-00004.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.00G/5.00G [01:27<00:00, 57.4MB/s]
Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:27<00:00, 21.80s/it]
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set max_memory in to a higher value to use more memory (at your own risk).s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:12<00:00,  3.16s/it]
generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00<00:00, 1.75MB/s]
Some parameters are on the meta device because they were offloaded to the cpu.
2025-11-14 21:14:17.177 | INFO     | __main__:<module>:43 - âš™ï¸ Initializing text generation pipeline...
Device set to use cuda:0
2025-11-14 21:14:17.221 | INFO     | __main__:main:102 - ğŸ”¥ Running Local LLM API on port 8000
INFO:     Started server process [23093]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2025-11-14 21:17:21.566 | INFO     | __main__:infer:80 - ğŸ§  Generating response for: 
You are a content creator. Write what the user asked you to while using the pro...
2025-11-14 21:19:57.653 | INFO     | rag_llm_system.infrastructure.local.governance:log_inference:32 - [Governance] {'timestamp': '2025-11-14T21:19:57.653383', 'input': '\nYou are a content creator. Write what the user asked you to while using the provided context as the primary source of information for the content.\nUser query: The recent amendment of the agricultural agreement between Morocco and the European Union signifies a noteworthy development in international trade relations. This agreement confirms the applicability of preferential tariffs to Southern Provinces,\nContext: \n            ', 'output': "\nYou are a content creator. Write what the user asked you to while using the provided context as the primary source of information for the content.\nUser query: The recent amendment of the agricultural agreement between Morocco and the European Union signifies a noteworthy development in international trade relations. This agreement confirms the applicability of preferential tariffs to Southern Provinces,\nContext: \n             Morocco and the European Union (EU) have recently amended their agricultural agreement, which signifies a noteworthy development in international trade relations. This agreement confirms the applicability of preferential tariffs to the Southern Provinces, including the Western Sahara, as designated by the EU. This is a significant milestone for the EU, as it seeks to strengthen its trade relations with Morocco, a key strategic partner in the region. The agreement also underscores the importance of the EU's commitment to supporting the agricultural sector in Morocco, which is a major contributor to the country's economy. This amendment is expected to provide significant benefits to both parties, as it will facilitate greater market access for Moroccan agricultural products, while also ensuring that the EU can continue to access high-quality agricultural goods from Morocco. This agreement is a testament to the strong partnership between the EU and Morocco, and it is expected to have a positive impact on the economies of both countries, as well as the wider region.", 'status': 'compliant'}
INFO:     127.0.0.1:50124 - "POST /infer HTTP/1.1" 200 OK